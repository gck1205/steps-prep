##Apach Spark Code
https://storage.googleapis.com/cloud-training/dataengineering/lab_assets/idegc/dataproc-templates.zip

Avro file download:

wget https://storage.googleapis.com/cloud-training/dataengineering/lab_assets/idegc/campaigns.avro
#################
Spark Bigquery jar:

gs://cloud-training/dataengineering/lab_assets/idegc/spark-bigquery_2.12-20221021-2134.jar
################
  gcloud dataproc serverless batches submit pyspark \
        --project=your-gcp-project \
        --region=your-gcp-region \
        --batch=your-batch-name \
        --file=your-pyspark-script.py \
        --jars=gs://spark-lib/bigquery/spark-bigquery-with-dependencies_2.12-*.jar \
        --properties=spark.jars.packages=org.apache.spark:spark-avro_2.12:your-avro-version

########
Leveraging Jupyter notebooks to pull data from source systems to bigquery
Jupyter lab for serverless batch and notebook sessions

###############Delta lake vs Big Lake

Delta Lake is an open-source storage layer that adds ACID transactions and 
other performance and reliability features to the data, and it can be used with external tables

BigLake is a Google Cloud storage engine that provides a unified, 
managed view over data lakes and warehouses, offering fine-grained access control and 
high performance without requiring users to have direct access to the underlying storage

Iceberg is an open source table format that supports petabyte scale data tables. 
The Iceberg open specification lets you run multiple query engines on a single copy of data stored
in an object store. Apache Iceberg external tables (hereafter called Iceberg external tables) 
support Iceberg specification version 2, including merge-on-read.

CREATE EXTERNAL TABLE myexternal-table
  WITH CONNECTION `myproject.us.myconnection`
  OPTIONS (
         format = 'ICEBERG',
         uris = ["gs://mybucket/mydata/mytable/metadata/iceberg.metadata.json"]
   )

################### Use dataframes in dbt##########

dataframes can be used in dbt
#########

Datastream API 

Datastream helps in replication(CDC) of data from one system salesforce to BigQuery

##########
BQ Security:

Control Access with IAM
Control Access with Authorization
Restrict network access
Control Column and row access
Protect Sensitive data 
  --Data Policy
     roles/bigquerydatapolicy.admin
     CREATE[ OR REPLACE] DATA_POLICY [IF NOT EXISTS] `myproject.region-us.data_policy_name`
    OPTIONS (
      data_policy_type="DATA_MASKING_POLICY",
      masking_expression="ALWAYS_NULL"
    );

        GRANT FINE_GRAINED_READ ON DATA_POLICY `myproject.region-us.data_policy_name`
    TO "principal://goog/subject/user1@example.com","principal://goog/subject/user2@example.com";

    DROP DATA_POLICY `myproject.region-us.data_policy_name`; 

     CREATE TABLE myproject.table1 (
    name INT64 OPTIONS (data_policies=["{'name':'myproject.region-us.data_policy_name1'}",
                                      "{'name':'myproject.region-us.data_policy_name2'}"])
    );
 --------------------------------------------------------------------
    You can't apply both policy tags and data policies to the same column.
 ------------------------------------------------------
Manage encryption
----------------------
AEAD.ENCRYPT(
  KEYS.KEYSET_CHAIN(kms_resource_name, first_level_keyset),
  plaintext,
  additional_authenticated_data)
------------------

AEAD.DECRYPT_STRING(
  KEYS.KEYSET_CHAIN(kms_resource_name, first_level_keyset),
  ciphertext,
  additional_authenticated_data)



----------------------------
############
Load Data
############

LOAD DATA INTO ticket_sales.categories
(
  catid INT64 NOT NULL,
  catgroup STRING,
  catname STRING,
  catdesc STRING)
FROM FILES (
  format = 'CSV', 
  field_delimiter = ',',
  max_bad_records = 10,
  uris = ['gs://cloud-training/bigquery-demos/ticket-sales/categories.csv']);

  ####Partition By###############
  CREATE OR REPLACE TABLE
  ticket_sales.sales_partitioned_by_date
PARTITION BY
  DATETIME_TRUNC(saletime, DAY)
AS (
  SELECT
    * except (saletime), 
    PARSE_DATETIME( "%m/%d/%Y %H:%M:%S", saletime) as saletime
  FROM
    ticket_sales.sales );
######  ARRAY_AGG ##############
SELECT
  e.eventid,
  e.eventname,
  ARRAY_AGG(STRUCT(
  s.saletime,
  s.qtysold,
  s.pricepaid,
  s.commission)) as sales
FROM
  ticket_sales.events e
JOIN
  ticket_sales.sales s
  on
  e.eventid = s.eventid
GROUP BY eventid, eventname
ORDER BY eventid, eventname;
#########  UNNEST ###########

the UNNEST function is used to flatten the sales array. 
Then, it can be queried and those results are converted into an array.
######

SELECT
  eventid,
  eventname,
  ARRAY((SELECT AS STRUCT saletime, commission FROM UNNEST(sales)
   ORDER BY(commission) DESC LIMIT 2)) as top_2
FROM
  ticket_sales.event_sales
ORDER BY eventid;

=================
SELECT
event.event_date,
event_name,
geo.country,
item.item_name
from `thelook_gcda.ga4_events` as event
INNER JOIN unnest(items) as item
WHERE
event_name = 'view_item'
AND geo.country = 'Singapore'
AND item.item_name = 'Google Dino Game Tee';