#
Implementing data security involves encryption, access controls, user authentication, regular audits, and employing secure coding practices to protect data from unauthorized access and breaches.

data catalogue:
Linking of Technical metadata with --> Business metadata using Tags

1) First create tag template :
    Add column details such as source,volume,has_pii, pii_type

2) Apply policy on top of the tags/tag templates:
    policy tags can be applied at  column level
  Taxonomies needs to be defined before applying policy tags
  Eg:
  Highly Confidential -- SSN

  Medium Confidential - Address

3) set IAM permissions based on policy tags

4) Users without access cannot query restricted columns

Taxonomy - its a container where you organise policy tags into a hierarchy
In BQ table, under Edit Schema : Add policy Tag --> assigning as highly confidentail or Medium Confidential

Row level restriction -- policy

 create row  access policy "mark_access" on demo.employ  grant to (<email>) filter using (department="Marketing").

 This gives only rows from table where department="Marketing"
 --------BQ Billing------
Storage Pricing:

 Logical --> actual data size without compression

 Physical --> data is compressed and stored on disk

 Billing type for a dataset can be checked by looking into parameter: storage_billing_parameter  -->where physical/Logical is selected 

 Compute Pricing:

 select * from demo.products limit 1 --> processes 18.75GB
 select id,product  from demo.products --> processes 600MB   

 Since BQ is a columnar db '*' should be avoided since it picks all columns data irrespective of limit 1 

 OnDemand pricing:
     charged per TB of data processed
      automatic scaling

 Flat rate pricing: 
     purchase dedicated processing capacity known as slots at flat monthly rate     

Federated Queries:
 can connect from BQ to other sources such as Cloud SQL,Spanner,SAP(beta)
 can connect using "external_query"
select * from external_query("bigdata_lake","select * from employee")\
 
 BigLake table:
 access datasources and datawarehouses 
 
 biglake table vs external table
   when shared with other users --> cloud storage access needs to be given for external table whereas not required for biglake table

Bigquery Omni -> used to query data from AWS S3/ Azure blob

Partition table:

create table orders_p partition by date(creation_date) cluster by (tags) as
select * from orders where creation_date >= date('2020-01-01')

#############################
Ensuring high availability and disaster recovery in a cloud-based database system involves using techniques such as multi-region deployments, automated backups, and replication. 

Multi-region deployments distribute data across different geographical locations to mitigate the impact of regional outages. Automated backups ensure that data can be restored to a previous state in case of failures. 

Replication keeps multiple copies of data synchronized across different nodes, providing redundancy and enabling quick failover in case of primary node failure.